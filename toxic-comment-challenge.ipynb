{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used https://github.com/wongchunghang/toxic-comment-challenge-lstm/blob/master/toxic_comment_9872_model.ipynb for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Conv1D, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import LSTM, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers import MaxPool1D, Concatenate, Flatten\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "from keras import optimizers\n",
    "from keras.layers import Lambda\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "import time\n",
    "\n",
    "from IPython.display import Image\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "1. Remove non-ascii characters\n",
    "2. Correct misspelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_character_removal=re.compile(r'[^a-z\\?\\!\\#\\@\\%\\* ]',re.IGNORECASE)\n",
    "def clean_text(x):\n",
    "    x_ascii = unidecode(x)\n",
    "    x_clean = special_character_removal.sub('',x_ascii)\n",
    "    return x_clean\n",
    "\n",
    "train['clean_text'] = train['comment_text'].apply(lambda x: clean_text(str(x)))\n",
    "test['clean_text'] = test['comment_text'].apply(lambda x: clean_text(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Word Count'] =  train['clean_text'].apply(lambda x: len(x.split (' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 1\n",
      "Median: 36.0\n",
      "Max: 2273\n"
     ]
    }
   ],
   "source": [
    "print ('Min:', train['Word Count'].min())\n",
    "print ('Median:', train['Word Count'].median())\n",
    "print ('Max:', train['Word Count'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train['clean_text']\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "X_test = test['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457142\n"
     ]
    }
   ],
   "source": [
    "# For best score (Public: 9869, Private: 9865), change to max_features = 283759, maxlen = 900\n",
    "max_features = 40000 #top_words\n",
    "maxlen = 500 #text_len\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train_sequence = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequence = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "x_train = sequence.pad_sequences(X_train_sequence, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test_sequence, maxlen=maxlen)\n",
    "print(len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts[list(tokenizer.word_index.keys())[50000]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use fasttext embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FastText Web Crawl vectors\n",
    "EMBEDDING_FILE_FASTTEXT=\"../data/fasttesxt/crawl-300d-2M.vec\"\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index_ft = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE_FASTTEXT,encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell_model = gensim.models.KeyedVectors.load_word2vec_format(EMBEDDING_FILE_FASTTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is  based on: Spellchecker using Word2vec by CPMP\n",
    "# https://www.kaggle.com/cpmpml/spell-checker-using-word2vec\n",
    "\n",
    "words = spell_model.index2word\n",
    "\n",
    "w_rank = {}\n",
    "for i,word in enumerate(words):\n",
    "    w_rank[word] = i\n",
    "\n",
    "WORDS = w_rank\n",
    "\n",
    "# Use fast text as vocabulary\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def P(word): \n",
    "    \"Probability of `word`.\"\n",
    "    # use inverse of rank as proxy\n",
    "    # returns 0 if the word isn't in the dictionary\n",
    "    return - WORDS.get(word, 0)\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "def singlify(word):\n",
    "    return \"\".join([letter for i,letter in enumerate(word) if i == 0 or letter != word[i-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words,301))\n",
    "\n",
    "something_ft = embeddings_index_ft.get(\"something\")\n",
    "\n",
    "something = np.zeros((301,))\n",
    "something[:300,] = something_ft\n",
    "something[300,] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove html tag like stylebackgroundcolor, verticalaligntop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_caps(word):\n",
    "    return len(word) > 1 and word.isupper()\n",
    "\n",
    "def embed_word(embedding_matrix,i,word):\n",
    "    embedding_vector_ft = embeddings_index_ft.get(word)\n",
    "    if embedding_vector_ft is not None: \n",
    "        if all_caps(word):\n",
    "            last_value = np.array([1])\n",
    "        else:\n",
    "            last_value = np.array([0])\n",
    "        embedding_matrix[i,:300] = embedding_vector_ft\n",
    "        embedding_matrix[i,300] = last_value\n",
    "\n",
    "            \n",
    "# Fasttext vector is used by itself if there is no glove vector but not the other way around.\n",
    "for word, i in word_index.items():\n",
    "    \n",
    "    if i >= max_features: continue\n",
    "        \n",
    "    if embeddings_index_ft.get(word) is not None:\n",
    "        embed_word(embedding_matrix,i,word)\n",
    "    else:\n",
    "        # change to > 20 for better score.\n",
    "        if len(word) > 20:\n",
    "            embedding_matrix[i] = something\n",
    "        else:\n",
    "            word2 = correction(word)\n",
    "            if embeddings_index_ft.get(word2) is not None:\n",
    "                embed_word(embedding_matrix,i,word2)\n",
    "            else:\n",
    "                word2 = correction(singlify(word))\n",
    "                if embeddings_index_ft.get(word2) is not None:\n",
    "                    embed_word(embedding_matrix,i,word2)\n",
    "                else:\n",
    "                    embedding_matrix[i] = something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "def get_model(clipvalue=0.5,num_filters=40,dropout=0.5,embed_size=301):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    \n",
    "    # Layer 1: concatenated fasttext and glove twitter embeddings.\n",
    "    embedding = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    \n",
    "    # Layer 2: SpatialDropout1D(0.5)\n",
    "    x = SpatialDropout1D(dropout)(embedding)\n",
    "    \n",
    "    # Layer 3: Convolutional layer\n",
    "    conv_0 = Conv1D(64, kernel_size=1, padding='valid', kernel_initializer='normal', activation='relu')(x)\n",
    "    conv_1 = Conv1D(64, kernel_size=2, padding='valid', kernel_initializer='normal', activation='relu')(x)\n",
    "    conv_2 = Conv1D(64, kernel_size=3, padding='valid', kernel_initializer='normal', activation='relu')(x)\n",
    "    conv_3 = Conv1D(64, kernel_size=4, padding='valid', kernel_initializer='normal', activation='relu')(x)\n",
    "    \n",
    "    # Layer 4: max pooling (change to K-max pooling)\n",
    "    maxpool_0 = MaxPool1D(pool_size=(maxlen - 1 + 1), strides=1, padding='valid')(conv_0)\n",
    "    maxpool_1 = MaxPool1D(pool_size=(maxlen - 2 + 1), strides=1, padding='valid')(conv_1)\n",
    "    maxpool_2 = MaxPool1D(pool_size=(maxlen - 3 + 1), strides=1, padding='valid')(conv_2)\n",
    "    maxpool_3 = MaxPool1D(pool_size=(maxlen - 4 + 1), strides=1, padding='valid')(conv_3)\n",
    "    \n",
    "    # Layer 5: Concatenate\n",
    "    concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2, maxpool_3])\n",
    "    flatten = Flatten()(concatenated_tensor)\n",
    "    \n",
    "    # Layer 6: Dropout\n",
    "    drop = Dropout(dropout)(flatten)\n",
    "    \n",
    "    # Layer 7: Dense layer\n",
    "    dense_1 = Dense(units=20, activation='relu')(drop)\n",
    "    \n",
    "    # Layer 8: Dense layer\n",
    "    output = Dense(units=6, activation='sigmoid')(dense_1)\n",
    "    model = Model(inputs=[inp], outputs=output)\n",
    "    \n",
    "    # compile\n",
    "    adam = optimizers.adam(clipvalue=clipvalue)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 500, 301)     12040000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 500, 301)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 500, 64)      19328       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 499, 64)      38592       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 498, 64)      57856       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 497, 64)      77120       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1, 64)        0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1, 64)        0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 1, 64)        0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1, 64)        0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4, 64)        0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "                                                                 max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 256)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 20)           5140        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 6)            126         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 12,238,162\n",
      "Trainable params: 198,162\n",
      "Non-trainable params: 12,040,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "get_model = get_model()\n",
    "get_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(get_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "# Image('model_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "159571/159571 [==============================] - 138s 864us/step - loss: 0.1054 - acc: 0.9656\n",
      "Epoch 2/10\n",
      "159571/159571 [==============================] - 136s 853us/step - loss: 0.0632 - acc: 0.9784\n",
      "Epoch 3/10\n",
      "159571/159571 [==============================] - 136s 853us/step - loss: 0.0578 - acc: 0.9796\n",
      "Epoch 4/10\n",
      "159571/159571 [==============================] - 136s 853us/step - loss: 0.0553 - acc: 0.9802\n",
      "Epoch 5/10\n",
      "159571/159571 [==============================] - 136s 853us/step - loss: 0.0536 - acc: 0.9806\n",
      "Epoch 6/10\n",
      "159571/159571 [==============================] - 136s 853us/step - loss: 0.0524 - acc: 0.9806\n",
      "Epoch 7/10\n",
      "159571/159571 [==============================] - 136s 853us/step - loss: 0.0511 - acc: 0.9809\n",
      "Epoch 8/10\n",
      "159571/159571 [==============================] - 136s 853us/step - loss: 0.0503 - acc: 0.9812\n",
      "Epoch 9/10\n",
      "159571/159571 [==============================] - 136s 853us/step - loss: 0.0499 - acc: 0.9813\n",
      "Epoch 10/10\n",
      "159571/159571 [==============================] - 136s 853us/step - loss: 0.0491 - acc: 0.9814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc3fb8993c8>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NN batch size\n",
    "NN_batch_size = 512\n",
    "\n",
    "# Number of NN epochs\n",
    "NN_epochs = 10\n",
    "get_model.fit(x_train, y_train,batch_size=NN_batch_size, epochs=NN_epochs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = get_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "output=pd.DataFrame(data=proba, index=test[\"id\"])\n",
    "output.to_csv(\"./output/test_cnn_2.csv\",header=[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "              ,index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
