{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fork of https://www.kaggle.com/antmarakis/bi-lstm-conv-layer?scriptVersionId=2789290\n",
    "Just replaced the data with Preprocessed data\n",
    "Public LB score 0.9833 => 0.9840\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, Input, LSTM, Bidirectional, Conv1D, CuDNNLSTM\n",
    "from keras.layers import Dropout, Embedding\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "import xgboost as xgb\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_character_removal=re.compile(r'[^a-z\\?\\!\\#\\@\\%\\* ]',re.IGNORECASE)\n",
    "def clean_text(x):\n",
    "    x_ascii = unidecode(x)\n",
    "    x_clean = special_character_removal.sub('',x_ascii)\n",
    "    return x_clean\n",
    "\n",
    "train['clean_text'] = train['comment_text'].apply(lambda x: clean_text(str(x)))\n",
    "test['clean_text'] = test['comment_text'].apply(lambda x: clean_text(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features=100000\n",
    "maxlen=150\n",
    "embed_size=300\n",
    "\n",
    "# max_features=50000\n",
    "# maxlen=900\n",
    "# embed_size=301\n",
    "\n",
    "train_y = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values\n",
    "train_x = train['clean_text']#.str.lower()\n",
    "\n",
    "test_x = test['clean_text']#.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize text + Prepare fasttext Embedding\n",
    "tokenizer = text.Tokenizer(num_words=max_features, lower=True)\n",
    "tokenizer.fit_on_texts(list(train_x))\n",
    "\n",
    "train_x = tokenizer.texts_to_sequences(train_x)\n",
    "test_x = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "train_x = sequence.pad_sequences(train_x, maxlen=maxlen)\n",
    "test_x = sequence.pad_sequences(test_x, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in train_x[0]:\n",
    "#     print(list(tokenizer.word_index.keys())[list(tokenizer.word_index.values()).index(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE=\"../data/fasttext/crawl-300d-2M.vec\"\n",
    "embeddings_index = {}\n",
    "with open(EMBEDDING_FILE, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(max_features, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embed_size))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "inp = Input(shape=(maxlen,))\n",
    "\n",
    "embedding_layer = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "x = SpatialDropout1D(0.35)(embedding_layer)\n",
    "\n",
    "x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n",
    "#x = Dropout(0.3)(x) #dropout doesnt improve\n",
    "x = Conv1D(64, kernel_size=3, padding='valid', kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "avg_pool = GlobalAveragePooling1D()(x)\n",
    "max_pool = GlobalMaxPooling1D()(x)\n",
    "x = concatenate([avg_pool, max_pool])\n",
    "\n",
    "out = Dense(6, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inp, out)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.2757 -0.0343  0.1668  0.0358 -0.0805 -0.0105  0.0752  0.324   0.1245\n",
      " -0.002  -0.0176  0.3054 -0.0302  0.2219 -0.1233 -0.1776 -0.3783 -0.0099\n",
      " -0.0945  0.1197 -0.0771 -0.2172 -0.0581 -0.2592 -0.0912  0.207   0.0356\n",
      "  0.1817 -0.1424  0.0026 -0.2029 -0.0429  0.0164 -0.3866 -0.0228 -0.1913\n",
      "  0.025   0.0919  0.1341  0.28   -0.1914 -0.0225  0.0942 -0.0417 -0.0278\n",
      " -0.0698 -0.1828 -0.0449  0.0688  0.14   -0.0579 -0.1856  0.1317  0.0861\n",
      " -0.2081 -0.0542  0.0502 -0.0092 -0.1887 -0.0216  0.0347 -0.269  -0.1502\n",
      "  0.3031  0.0336  0.159   0.1326 -0.1242 -0.1914 -0.2266  0.1209 -0.0246\n",
      " -0.1972  0.3093 -0.0582  0.1337 -0.0827 -0.0721  0.3924 -0.2054  0.2582\n",
      "  0.18   -0.2151  0.0502 -0.3227  0.0237 -0.0227  0.2881  0.0154 -0.1839\n",
      " -0.1311 -0.0507 -0.157   0.1108 -0.168  -0.3899 -0.0335 -0.0088 -0.1911\n",
      "  0.1927 -0.0023  0.105   0.0059  0.2258  0.5905 -0.1266 -0.1993 -0.2283\n",
      " -0.274  -0.1581 -0.2229 -0.1595  0.1269  0.0143 -0.7361  0.0523  0.2621\n",
      "  0.0669 -0.1305 -0.0139  0.0975 -0.3305 -0.0738  0.0065 -0.0016 -0.1928\n",
      " -0.1817 -0.2002  0.3678  0.0307  0.0808 -0.0033 -0.0334  0.0557  0.2133\n",
      " -0.3348  0.0249  0.1442  0.2259 -0.1075 -0.0251  0.0289 -0.0144  0.2411\n",
      "  0.1675  0.0404 -0.0299 -0.1613  0.0904 -0.3078  0.0896 -0.155  -0.0329\n",
      " -0.0656 -0.1783  0.0259 -0.1238  0.0758  0.026  -0.0028  0.2884 -0.2789\n",
      "  0.184   0.0359  0.2156  0.0207  0.0306  0.1075  0.1728 -0.0798 -0.0603\n",
      "  0.0094  0.2441 -0.0557  0.294   0.0743 -0.0547  0.0468  0.0914  0.2759\n",
      " -0.0606  0.0181  0.1746 -0.106   0.2294 -0.0644  0.0758  0.1209  0.2587\n",
      " -0.0657 -0.0105 -0.1972 -0.1599  0.0357  0.0317  0.1543 -0.24    0.0797\n",
      " -0.1335 -0.0866 -0.1507 -0.0936  0.274  -0.14    0.0987  0.1088 -0.0023\n",
      " -0.1748  0.1281 -0.1482  0.2536  0.5858 -0.1214  0.3646 -0.1812 -0.3778\n",
      " -0.1131 -0.2791 -0.0535 -0.0952 -0.234   0.4323  0.0725  0.0256 -0.1214\n",
      "  0.0982  0.0318  0.0861  0.0755  0.2031 -0.1343  0.0734 -0.2278 -0.1465\n",
      " -0.2979 -0.8204  0.0539  0.1315 -0.0743 -0.1964  0.1012  0.053   0.4443\n",
      "  0.0084  0.2163 -0.0692  0.1699 -0.1831 -0.1577  0.1179 -0.1101 -0.2984\n",
      "  0.2195 -0.2743  0.0925  0.0715  0.0984 -0.1626 -0.2638  0.0292  0.1296\n",
      "  0.3439  0.1759  0.1203 -0.0697  0.3011  0.0398 -0.1627 -0.0367 -0.0654\n",
      " -0.0363 -0.4255 -0.1171 -0.0378 -0.1494  0.1718 -0.3368  0.0823 -0.1915\n",
      "  0.0066 -0.0847 -0.0652 -0.0337  0.037  -0.2324 -0.281   0.0549  0.1209\n",
      " -0.1251  0.1857 -0.1409 -0.0767 -0.028   0.0769 -0.143  -0.1704  0.0427\n",
      " -0.0844  0.1493 -0.0507]\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.layers[1].get_weights()[0]\n",
    "\n",
    "# or access the embedding layer through the constructed model \n",
    "# first `0` refers to the position of embedding layer in the `model`\n",
    "embeddings = model.layers[1].get_weights()[0]\n",
    "\n",
    "# `embeddings` has a shape of (num_vocab, embedding_dim) \n",
    "\n",
    "# `word_to_index` is a mapping (i.e. dict) from words to their index, e.g. `love`: 69\n",
    "words_embeddings = {w:embeddings[idx] for w, idx in word_index.items()  if idx < 100000}\n",
    "\n",
    "# now you can use it like this for example\n",
    "print(words_embeddings['love'])  # possible output: [0.21, 0.56, ..., 0.65, 0.10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x[0]\n",
    "def sent2vec(s):\n",
    "    M = []\n",
    "    for index in train_x[0]:\n",
    "        if index != 0:\n",
    "            M.append(words_embeddings[res[index]])\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "           return np.zeros(300)\n",
    "    return(v / np.sqrt((v ** 2).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_fasttext = [sent2vec(x) for x in train_x]\n",
    "xtest_fasttext = [sent2vec(x) for x in test_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_fasttext = np.array(xtrain_fasttext)\n",
    "xtest_fasttext = np.array(xtest_fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runXGB(train_X, train_y, seed_val=2017, num_rounds=500):\n",
    "    param = {}\n",
    "    param['objective'] = 'binary:logistic'\n",
    "    #param['booster'] = \"gbtree\", \n",
    "    param['eta'] = 0.2\n",
    "    param['max_depth'] = 3\n",
    "    param['silent'] = 1\n",
    "    param['eval_metric'] = 'auc'\n",
    "    param['min_child_weight'] = 4\n",
    "    param['subsample'] = 0.7\n",
    "    param['colsample_bytree'] = 0.7\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "#     if test_y is not None:\n",
    "#         xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "#         watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "#         model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=20)\n",
    "#     else:\n",
    "        #xgtest = xgb.DMatrix(test_X)\n",
    "    model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit toxic\n",
      "fit severe_toxic\n",
      "fit obscene\n",
      "fit threat\n",
      "fit insult\n",
      "fit identity_hate\n"
     ]
    }
   ],
   "source": [
    "col = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "preds = np.zeros((test.shape[0], len(col)))\n",
    "\n",
    "for i, j in enumerate(col):\n",
    "    print('fit '+j)\n",
    "    model = runXGB(xtrain_fasttext, train_y[:,i])\n",
    "    preds[:,i] = model.predict(xgb.DMatrix(xtest_fasttext), ntree_limit = model.best_ntree_limit)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09606097, 0.00993686, 0.05294036, 0.00299561, 0.04932525,\n",
       "        0.00879266],\n",
       "       [0.09606097, 0.00993686, 0.05294036, 0.00299561, 0.04932525,\n",
       "        0.00879266],\n",
       "       [0.09606097, 0.00993686, 0.05294036, 0.00299561, 0.04932525,\n",
       "        0.00879266],\n",
       "       [0.09606097, 0.00993686, 0.05294036, 0.00299561, 0.04932525,\n",
       "        0.00879266],\n",
       "       [0.09606097, 0.00993686, 0.05294036, 0.00299561, 0.04932525,\n",
       "        0.00879266],\n",
       "       [0.09606097, 0.00993686, 0.05294036, 0.00299561, 0.04932525,\n",
       "        0.00879266],\n",
       "       [0.09606097, 0.00993686, 0.05294036, 0.00299561, 0.04932525,\n",
       "        0.00879266],\n",
       "       [0.09606097, 0.00993686, 0.05294036, 0.00299561, 0.04932525,\n",
       "        0.00879266],\n",
       "       [0.09606097, 0.00993686, 0.05294036, 0.00299561, 0.04932525,\n",
       "        0.00879266],\n",
       "       [0.09606097, 0.00993686, 0.05294036, 0.00299561, 0.04932525,\n",
       "        0.00879266]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "bad input shape (159571, 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-4ef5eef36973>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m           \u001b[0msubsample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m           colsample_bytree = 0.7, silent=False)\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain_fasttext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set)\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0mxgb_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"eval_metric\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_le\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m         \u001b[0mtraining_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0man\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mof\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \"\"\"\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bad input shape {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: bad input shape (159571, 6)"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on glove features\n",
    "clf = xgb.XGBClassifier(objective = \"binary:logistic\", \n",
    "          booster = \"gbtree\", \n",
    "          eval_metric = \"auc\", \n",
    "          #nthread = 4, \n",
    "          eta = 0.2, \n",
    "          max_depth = 3,\n",
    "          min_child_weight = 4,\n",
    "          subsample = 0.7,\n",
    "          colsample_bytree = 0.7, silent=False)\n",
    "clf.fit(xtrain_fasttext, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "159571/159571 [==============================] - 422s 3ms/step - loss: 0.0905 - acc: 0.9721\n",
      "Epoch 2/5\n",
      "159571/159571 [==============================] - 422s 3ms/step - loss: 0.0479 - acc: 0.9825\n",
      "Epoch 3/5\n",
      "159571/159571 [==============================] - 422s 3ms/step - loss: 0.0422 - acc: 0.9841\n",
      "Epoch 4/5\n",
      "159571/159571 [==============================] - 422s 3ms/step - loss: 0.0382 - acc: 0.9853\n",
      "Epoch 5/5\n",
      "159571/159571 [==============================] - 421s 3ms/step - loss: 0.0346 - acc: 0.9865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd119439860>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction\n",
    "batch_size = 512\n",
    "epochs = 5\n",
    "\n",
    "model.fit(train_x, train_y, batch_size=batch_size, epochs=epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 127s 831us/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_x, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "output=pd.DataFrame(data=predictions, index=test[\"id\"])\n",
    "output.to_csv(\"./output/bilstm_conv_embedding_matrix.csv\",header=[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "              ,index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
