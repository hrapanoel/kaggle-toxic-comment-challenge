{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "242ec0d79a7869d955f02cb42e5513ed01421227"
      },
      "cell_type": "code",
      "source": "import os\nprint(os.listdir(\"../input\"))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport string\nimport re\n\nfrom collections import Counter\nimport pickle\n\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Dropout, Conv1D, Embedding, SpatialDropout1D, concatenate\nfrom keras.layers import LSTM, GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom keras.layers import MaxPool1D, Concatenate, Flatten\nfrom keras.preprocessing import text, sequence\n\nfrom keras.callbacks import Callback\nfrom keras import optimizers\nfrom keras.layers import Lambda\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\n\nfrom keras import backend as K\nfrom unidecode import unidecode\nimport time",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eed9c8ab2edfe33c281fde1dd49680d3ec85effe"
      },
      "cell_type": "code",
      "source": "import logging\nfrom sklearn.metrics import roc_auc_score\nfrom keras.callbacks import Callback\n\nclass RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "14b2fffb035d5c0e7da8bae161d5cbcb228ae0ea"
      },
      "cell_type": "code",
      "source": "# Load data\ntrain = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\ntest = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2b707339405c1cb4a39c59ef90de1b90bdd65172"
      },
      "cell_type": "markdown",
      "source": "## Preprocessing\n1. Remove non-ascii characters\n2. Correct misspelling"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d8234a290281e36b2cd12eebb24408e3146ccd6f"
      },
      "cell_type": "code",
      "source": "special_character_removal=re.compile(r'[^a-z\\?\\!\\#\\@\\%\\* ]',re.IGNORECASE)\ndef clean_text(x):\n    x_ascii = unidecode(x)\n    x_clean = special_character_removal.sub('',x_ascii)\n    return x_clean\n\ntrain['clean_text'] = train['comment_text'].apply(lambda x: clean_text(str(x)))\ntest['clean_text'] = test['comment_text'].apply(lambda x: clean_text(str(x)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "24464c5ccb48face6a3a494eceb7bd79ede75c49"
      },
      "cell_type": "code",
      "source": "X_train = train['clean_text']\ny_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\nX_test = test['clean_text']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d25c574a74ed9e1d1bdede491ce99a02712c3d4e"
      },
      "cell_type": "code",
      "source": "max_features = 50000 #top_words\nmaxlen = 900 #text_len\n\ntokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X_train) + list(X_test))\nX_train_sequence = tokenizer.texts_to_sequences(X_train)\nX_test_sequence = tokenizer.texts_to_sequences(X_test)\n\nx_train = sequence.pad_sequences(X_train_sequence, maxlen=maxlen)\nx_test = sequence.pad_sequences(X_test_sequence, maxlen=maxlen)\nprint(len(tokenizer.word_index))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "96374b9366718682ed6cacabc88b2d3e3c97d9ff"
      },
      "cell_type": "markdown",
      "source": "Load precomputed embedding matrix"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "05a1f90950726f83f31aca07ecaa218f433c3b15"
      },
      "cell_type": "code",
      "source": "embedding_matrix = np.load(\"../input/precomputed-embedding-matrix/embedding_matrix_50000_301.npy\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4cfb2975c47d8f34d234d612a7e3a75fc6bb78fe"
      },
      "cell_type": "code",
      "source": "from keras.callbacks import EarlyStopping, ModelCheckpoint#, LearningRateScheduler\nfrom keras.layers import GRU, BatchNormalization, Conv1D, MaxPooling1D\nfrom keras.layers import Bidirectional\n\nK.clear_session()\ndef get_model(clipvalue=0.5,dropout=0.3,embed_size=301):\n    inp = Input(shape=(maxlen, ))\n    \n    # Layer 1: fasttext embeddings.\n    embedding = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    \n    # Layer 2: SpatialDropout1D(0.5)\n    x = SpatialDropout1D(dropout)(embedding)\n    \n    # Layer 3: Bidirectional GRU + convolutional\n    gru_1 = Bidirectional(GRU(units=40, return_sequences = True, recurrent_dropout = 0.1))(x)\n    conv_1 = Conv1D(60, kernel_size=3, padding='valid', kernel_initializer='normal', activation='relu')(gru_1)\n    \n    # Layer 4: Bidirectional GRU + convolutional\n    gru_2 = Bidirectional(GRU(units=80, return_sequences = True, recurrent_dropout = 0.1))(x)\n    conv_2 = Conv1D(120, kernel_size=2, padding='valid', kernel_initializer='normal', activation='relu')(gru_2)\n    \n    # Max_pool + ave_pool\n    avg_pool_1 = GlobalAveragePooling1D()(conv_1)\n    max_pool_1 = GlobalMaxPooling1D()(conv_1)\n    \n    avg_pool_2 = GlobalAveragePooling1D()(conv_2)\n    max_pool_2 = GlobalMaxPooling1D()(conv_2)\n    \n    # Concatenate\n    x = concatenate([avg_pool_1, max_pool_1, avg_pool_2, max_pool_2])\n    \n    x = Dense(6, activation = \"sigmoid\")(x)\n    model = Model(inputs = inp, outputs = x)\n    \n    # compile\n    adam = optimizers.adam(clipvalue=clipvalue)\n    model.compile(loss = \"binary_crossentropy\", optimizer = adam, metrics = [\"accuracy\"])\n    return model    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3d66918067b00bd7f8342a6cc5a079925bb67a8c"
      },
      "cell_type": "code",
      "source": "get_model = get_model()\nget_model.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "784be32a85f7fdcd8fcec43f69e500993edf9827"
      },
      "cell_type": "code",
      "source": "file_path = \"best_model.hdf5\"\ncheck_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                              save_best_only = True, mode = \"min\")\n#ra_val = RocAucEvaluation(validation_data=(X_valid, Y_valid), interval = 1)\nearly_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 5)\n\nget_model.fit(x_train, y_train, batch_size = 512, epochs = 4, \n                        verbose = 1, callbacks = [ check_point, early_stop])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "10e01bd8889c38016d7b8b94f1ac3aebf5e89fe8"
      },
      "cell_type": "code",
      "source": "proba = get_model.predict(x_test, batch_size=512, verbose=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d32588ef918757943fbccfb91ca7f551827508fe"
      },
      "cell_type": "code",
      "source": "# Create submission file\noutput=pd.DataFrame(data=proba, index=test[\"id\"])\noutput.to_csv(\"global_average_pooling.csv\",header=[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n              ,index=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "90a7d512bc569bc3f8b5cea96ae00a8c46576ca7"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}